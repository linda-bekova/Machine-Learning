{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úkol č. 2 - Využití neuronových sítí\n",
    "\n",
    "  * **Deadline je do 18. 5. 2023, 23:59:59**, pokud odevzdáte úkol do 24. 5. 2023, 23:59:59, budete penalizování -12 body, pozdější odevzdání je bez bodu.\n",
    "  * V rámci tohoto úkolu musíte sestrojit vhodný model neuronové sítě pro vícetřídou klasifikaci.\n",
    "  * Část bodů získáte za správné vypracování a část bodů získáte za výslednou přesnost Vašeho modelu na evaluačních datech.\n",
    "    \n",
    "> **Úkoly jsou zadány tak, aby Vám daly prostor pro invenci. Vymyslet _jak přesně_ budete úkol řešit, je důležitou součástí zadání a originalita či nápaditost bude také hodnocena!**\n",
    "\n",
    "Využívejte buňky typu `Markdown` k vysvětlování Vašeho postupu. Za nepřehlednost budou strhávány body.\n",
    "\n",
    "## Zdroj dat\n",
    "\n",
    " * Zdrojem dat jsou soubory `train.csv` a `evaluate.csv`.\n",
    " * Jedná se o obrázky 32x32 pixelů ve stupních šedi, které byly nějakým způsobem vyrobeny z [Fashion Mnist datasetu](https://www.kaggle.com/datasets/zalando-research/fashionmnist).\n",
    " * Soubor `train.csv` obsahuje trénovací data.\n",
    " * Cílová (vysvětlovaná) proměnná se jmenuje **label**.\n",
    " * Soubor `evaluate.csv` obsahuje testovací data bez hodnot skutečných labelů.\n",
    "\n",
    "## Pokyny k vypracování (max 18 bodů)\n",
    "\n",
    "**Body zadání**, za jejichž (poctivé) vypracování získáte **18 bodů**:\n",
    "  * V notebooku načtěte data ze souboru `train.csv`. Vhodným způsobem si je rozdělte na podmnožiny, které Vám poslouží pro trénování, porovnávání modelů a následnou predikci výkonnosti finálního modelu.\n",
    "  * Proveďte základní průzkum dat a svá pozorování diskutujte. Některé obrázky také zobrazte.\n",
    "  * Sestrojte a natrénujte několik variant modelu dopředné neuronové sítě. Přitom v rámci výpočetních možností:\n",
    "      * Okomentujte vhodnost daného modelu pro daný typ úlohy.\n",
    "      * Experimentujte s různými hloubkami a velikosmi vrstev.\n",
    "      * Experimentujte se standardizací/normalizací dat.\n",
    "      * Experimentujte s různými optimalizačními metodami.\n",
    "      * Experimentujte s různými regularizačními technikami.\n",
    "      * Získané výsledky vždy řádně okomentujte.\n",
    "\n",
    "  * Sestrojte model konvoluční neuronové sítě. Přitom v rámci výpočetních možností:\n",
    "      * Okomentujte vhodnost daného modelu pro daný typ úlohy.\n",
    "      * Experimentujte s různými hloubkami a velikosmi vrstev.\n",
    "      * Experimentujte se standardizací/normalizací dat.\n",
    "      * Experimentujte s různými optimalizačními metodami.\n",
    "      * Experimentujte s různými regularizačními technikami.\n",
    "      * Získané výsledky vždy řádně okomentujte.\n",
    "    \n",
    "  * Ze všech zkoušených možností vyberte finální model a odhadněte, jakou přesnost můžete očekávat na nových datech, která jste doposud neměli k dispozici.\n",
    "  \n",
    "  * Nakonec načtěte vyhodnocovací data ze souboru`evaluate.csv`. Pomocí finálního modelu napočítejte predikce pro tyto data (vysvětlovaná proměnná v nich již není). Vytvořte soubor `results.csv`, ve kterém získané predikce uložíte do dvou sloupců: **ID**, **predikce labelu**. Tento soubor též odevzdejte (uložte do projektu vedle notebooku).\n",
    "   \n",
    "   * Ukázka prvních řádků souboru `results.csv`:\n",
    "  \n",
    "```\n",
    "ID,label\n",
    "0,0\n",
    "1,1\n",
    "...\n",
    "```\n",
    "\n",
    "## Vyhodnocovací část (max 7 bodů)\n",
    "Za přesnost (accuraccy) na odevzdaných predikcích pro vyhodnocovací množnu získáte dalších max **7 bodů**.\n",
    "\n",
    "Označíme-li $A$ přesnost, které jste dosáhli, zaokrouhlenou na 2 desetinná místa, akumulují se výsledné body podle následujících pravidel:\n",
    "* pokud $A \\geq 0.80$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.83$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.86$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.87$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.88$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.89$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.90$ obdržíte +1 bod\n",
    "\n",
    "**Příklad:** Pokud bude Vaše přesnost 0.856, vyjde A = 0.86 a vy získáte 3 body.\n",
    "\n",
    "\n",
    "## Poznámky k odevzdání\n",
    "\n",
    "  * Řiďte se pokyny ze stránky https://courses.fit.cvut.cz/BI-ML2/homeworks/index.html.\n",
    "  * Vytvořte i csv soubor `results.csv` s predikcemi a uložte ho v rámci projektu vedle ipython notebooku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>pix10</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  pix10  ...  \\\n",
       "0         1     1     1     1     1     1     1     1     1      1  ...   \n",
       "1         4     4     4     4     4     4     5     5     5      6  ...   \n",
       "2         0     0     0     0     0     0     0     0     0      0  ...   \n",
       "3         2     2     2     2     2     2     2     2     2      2  ...   \n",
       "4         0     0     0     0     0     0     0     0     0      0  ...   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   \n",
       "55995     1     1     1     1     1     1     1     1     1      1  ...   \n",
       "55996     5     5     5     5     5     5     5     5     5      5  ...   \n",
       "55997     0     0     0     0     0     0     0     0     0      0  ...   \n",
       "55998     0     0     0     0     0     0     0     0     0      0  ...   \n",
       "55999     0     0     0     0     0     0     0     0     0      0  ...   \n",
       "\n",
       "       pix1016  pix1017  pix1018  pix1019  pix1020  pix1021  pix1022  pix1023  \\\n",
       "0            1        1        1        1        1        1        1        1   \n",
       "1            4        4        4        4        4        4        4        4   \n",
       "2            0        0        0        0        0        0        0        0   \n",
       "3            2        2        2        2        2        2        2        2   \n",
       "4            0        0        0        0        0        0        0        0   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "55995        1        1        1        1        1        1        1        1   \n",
       "55996        5        5        5        5        5        5        5        5   \n",
       "55997        0        0        0        0        0        0        0        0   \n",
       "55998        0        0        0        0        0        0        0        0   \n",
       "55999        0        0        0        0        0        0        0        0   \n",
       "\n",
       "       pix1024  label  \n",
       "0            1      7  \n",
       "1            4      0  \n",
       "2            0      6  \n",
       "3            2      3  \n",
       "4            0      3  \n",
       "...        ...    ...  \n",
       "55995        1      7  \n",
       "55996        5      9  \n",
       "55997        0      2  \n",
       "55998        0      3  \n",
       "55999        0      3  \n",
       "\n",
       "[56000 rows x 1025 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Columns: 1025 entries, pix1 to label\n",
      "dtypes: int64(1025)\n",
      "memory usage: 437.9 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info()\n",
    "dataset.isna().sum().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset.max() > 255).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset.min() < 0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = dataset.drop(columns=['label'])\n",
    "images = [dX.loc[i].array.reshape((32, 32)) for i in range(min(len(dX), 6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img in images:\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['label']), dataset.label, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [0.8, 0.2], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9604/622388474.py:3: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_y = torch.tensor(y_train.values.astype(np.long))\n",
      "/tmp/ipykernel_9604/622388474.py:8: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val_y = torch.tensor(y_val.values.astype(np.long))\n",
      "/tmp/ipykernel_9604/622388474.py:13: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_y = torch.tensor(y_test.values.astype(np.long))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "train_y = torch.tensor(y_train.values.astype(np.long))\n",
    "train_x = torch.tensor(x_train.values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train_x, train_y) \n",
    "trainloader = data_utils.DataLoader(dataset = train_tensor, batch_size = 64, shuffle = True)\n",
    "\n",
    "val_y = torch.tensor(y_val.values.astype(np.long))\n",
    "val_x = torch.tensor(x_val.values.astype(np.float32)) \n",
    "val_tensor = data_utils.TensorDataset(val_x, val_y) \n",
    "validloader = data_utils.DataLoader(dataset = val_tensor, batch_size = 64, shuffle = True)\n",
    "\n",
    "test_y = torch.tensor(y_test.values.astype(np.long))\n",
    "test_x = torch.tensor(x_test.values.astype(np.float32)) \n",
    "test_tensor = data_utils.TensorDataset(test_x, test_y) \n",
    "testloader = data_utils.DataLoader(dataset = test_tensor, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024]) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 784)\n",
    "        self.fc2 = nn.Linear(784, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fco = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "    \n",
    "model = MyNet()\n",
    "\n",
    "# zkusíme tam dát data\n",
    "outputs = model(images)\n",
    "print(images.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 38.68885040283203\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 2.297\n",
      "  batch 200 loss: 2.329\n",
      "  batch 300 loss: 2.292\n",
      "  batch 400 loss: 2.303\n",
      "  batch 500 loss: 2.305\n",
      "Celková trénovací chyba: 2.439965665908087\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer, trainloader):\n",
    "    running_cum_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        last_mean_loss = loss.item()\n",
    "        running_cum_loss += last_mean_loss * inputs.shape[0]\n",
    "        if i % 100 == 99:\n",
    "            print(f\"  batch {i+1} loss: {last_mean_loss:.3f}\")\n",
    "            \n",
    "    # Return of the average over the whole training set\n",
    "    return running_cum_loss / len(x_train)\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, trainloader)\n",
    "print(f\"Celková trénovací chyba: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "# 16384\n",
      "fc1.bias\n",
      "# 16\n",
      "fco.weight\n",
      "# 160\n",
      "fco.bias\n",
      "# 10\n",
      "\n",
      "Celkový počet parametrů: 16570\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}\\n# {param.numel()}\")\n",
    "    num_params += param.numel()\n",
    "    \n",
    "print(f\"\\nCelkový počet parametrů: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 2.300\n",
      "  batch 200 loss: 2.307\n",
      "  batch 300 loss: 2.264\n",
      "  batch 400 loss: 2.301\n",
      "  batch 500 loss: 2.308\n",
      "TRAIN loss: 2.296, VALIDATION loss: 2.279, accuraccy: 0.112\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 2.303\n",
      "  batch 200 loss: 2.269\n",
      "  batch 300 loss: 2.265\n",
      "  batch 400 loss: 2.234\n",
      "  batch 500 loss: 2.372\n",
      "TRAIN loss: 2.275, VALIDATION loss: 2.260, accuraccy: 0.120\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 2.195\n",
      "  batch 200 loss: 2.184\n",
      "  batch 300 loss: 2.156\n",
      "  batch 400 loss: 2.053\n",
      "  batch 500 loss: 1.896\n",
      "TRAIN loss: 2.082, VALIDATION loss: 1.969, accuraccy: 0.204\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 1.943\n",
      "  batch 200 loss: 1.908\n",
      "  batch 300 loss: 1.821\n",
      "  batch 400 loss: 1.843\n",
      "  batch 500 loss: 1.983\n",
      "TRAIN loss: 1.957, VALIDATION loss: 1.897, accuraccy: 0.218\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 1.885\n",
      "  batch 200 loss: 1.821\n",
      "  batch 300 loss: 1.856\n",
      "  batch 400 loss: 1.884\n",
      "  batch 500 loss: 2.070\n",
      "TRAIN loss: 1.863, VALIDATION loss: 1.853, accuraccy: 0.219\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 1.774\n",
      "  batch 200 loss: 1.914\n",
      "  batch 300 loss: 1.749\n",
      "  batch 400 loss: 1.813\n",
      "  batch 500 loss: 2.064\n",
      "TRAIN loss: 1.813, VALIDATION loss: 1.808, accuraccy: 0.226\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 1.661\n",
      "  batch 200 loss: 1.734\n",
      "  batch 300 loss: 1.758\n",
      "  batch 400 loss: 1.730\n",
      "  batch 500 loss: 1.691\n",
      "TRAIN loss: 1.771, VALIDATION loss: 1.759, accuraccy: 0.232\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 1.704\n",
      "  batch 200 loss: 1.597\n",
      "  batch 300 loss: 1.790\n",
      "  batch 400 loss: 1.936\n",
      "  batch 500 loss: 1.802\n",
      "TRAIN loss: 1.749, VALIDATION loss: 1.747, accuraccy: 0.242\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 1.746\n",
      "  batch 200 loss: 1.741\n",
      "  batch 300 loss: 1.707\n",
      "  batch 400 loss: 1.668\n",
      "  batch 500 loss: 1.660\n",
      "TRAIN loss: 1.735, VALIDATION loss: 1.748, accuraccy: 0.240\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 1.818\n",
      "  batch 200 loss: 1.741\n",
      "  batch 300 loss: 1.766\n",
      "  batch 400 loss: 1.800\n",
      "  batch 500 loss: 1.685\n",
      "TRAIN loss: 1.726, VALIDATION loss: 1.738, accuraccy: 0.247\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 1.743\n",
      "  batch 200 loss: 1.681\n",
      "  batch 300 loss: 1.704\n",
      "  batch 400 loss: 1.687\n",
      "  batch 500 loss: 1.874\n",
      "TRAIN loss: 1.717, VALIDATION loss: 1.750, accuraccy: 0.197\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 1.678\n",
      "  batch 200 loss: 1.682\n",
      "  batch 300 loss: 1.676\n",
      "  batch 400 loss: 1.637\n",
      "  batch 500 loss: 1.756\n",
      "TRAIN loss: 1.713, VALIDATION loss: 1.718, accuraccy: 0.243\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 1.794\n",
      "  batch 200 loss: 1.677\n",
      "  batch 300 loss: 1.685\n",
      "  batch 400 loss: 1.723\n",
      "  batch 500 loss: 1.594\n",
      "TRAIN loss: 1.708, VALIDATION loss: 1.712, accuraccy: 0.244\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 1.788\n",
      "  batch 200 loss: 1.678\n",
      "  batch 300 loss: 1.744\n",
      "  batch 400 loss: 1.724\n",
      "  batch 500 loss: 1.814\n",
      "TRAIN loss: 1.707, VALIDATION loss: 1.701, accuraccy: 0.245\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 1.641\n",
      "  batch 200 loss: 1.620\n",
      "  batch 300 loss: 1.684\n",
      "  batch 400 loss: 1.717\n",
      "  batch 500 loss: 1.741\n",
      "TRAIN loss: 1.704, VALIDATION loss: 1.711, accuraccy: 0.240\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 1.740\n",
      "  batch 200 loss: 2.126\n",
      "  batch 300 loss: 1.740\n",
      "  batch 400 loss: 1.666\n",
      "  batch 500 loss: 1.729\n",
      "TRAIN loss: 1.696, VALIDATION loss: 1.711, accuraccy: 0.242\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 1.671\n",
      "  batch 200 loss: 1.680\n",
      "  batch 300 loss: 1.656\n",
      "  batch 400 loss: 1.716\n",
      "  batch 500 loss: 1.731\n",
      "TRAIN loss: 1.701, VALIDATION loss: 1.707, accuraccy: 0.232\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 1.686\n",
      "  batch 200 loss: 1.599\n",
      "  batch 300 loss: 1.712\n",
      "  batch 400 loss: 1.752\n",
      "  batch 500 loss: 1.818\n",
      "TRAIN loss: 1.697, VALIDATION loss: 1.699, accuraccy: 0.241\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 1.706\n",
      "  batch 200 loss: 1.618\n",
      "  batch 300 loss: 1.616\n",
      "  batch 400 loss: 1.733\n",
      "  batch 500 loss: 1.667\n",
      "TRAIN loss: 1.696, VALIDATION loss: 1.711, accuraccy: 0.244\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 1.688\n",
      "  batch 200 loss: 1.630\n",
      "  batch 300 loss: 1.990\n",
      "  batch 400 loss: 1.612\n",
      "  batch 500 loss: 1.707\n",
      "TRAIN loss: 1.692, VALIDATION loss: 1.705, accuraccy: 0.238\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 1.684\n",
      "  batch 200 loss: 1.853\n",
      "  batch 300 loss: 1.646\n",
      "  batch 400 loss: 1.635\n",
      "  batch 500 loss: 1.699\n",
      "TRAIN loss: 1.691, VALIDATION loss: 1.705, accuraccy: 0.243\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 1.782\n",
      "  batch 200 loss: 1.615\n",
      "  batch 300 loss: 1.682\n",
      "  batch 400 loss: 1.885\n",
      "  batch 500 loss: 1.655\n",
      "TRAIN loss: 1.691, VALIDATION loss: 1.704, accuraccy: 0.242\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 1.717\n",
      "  batch 200 loss: 1.709\n",
      "  batch 300 loss: 1.706\n",
      "  batch 400 loss: 1.722\n",
      "  batch 500 loss: 1.673\n",
      "TRAIN loss: 1.688, VALIDATION loss: 1.703, accuraccy: 0.243\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 1.659\n",
      "  batch 200 loss: 1.654\n",
      "  batch 300 loss: 1.685\n",
      "  batch 400 loss: 1.749\n",
      "  batch 500 loss: 1.629\n",
      "TRAIN loss: 1.689, VALIDATION loss: 1.700, accuraccy: 0.243\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 1.697\n",
      "  batch 200 loss: 1.767\n",
      "  batch 300 loss: 1.578\n",
      "  batch 400 loss: 1.649\n",
      "  batch 500 loss: 1.604\n",
      "TRAIN loss: 1.689, VALIDATION loss: 1.703, accuraccy: 0.240\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 1.722\n",
      "  batch 200 loss: 1.758\n",
      "  batch 300 loss: 1.713\n",
      "  batch 400 loss: 1.561\n",
      "  batch 500 loss: 1.749\n",
      "TRAIN loss: 1.690, VALIDATION loss: 1.709, accuraccy: 0.231\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 1.670\n",
      "  batch 200 loss: 1.666\n",
      "  batch 300 loss: 1.692\n",
      "  batch 400 loss: 1.698\n",
      "  batch 500 loss: 1.639\n",
      "TRAIN loss: 1.688, VALIDATION loss: 1.698, accuraccy: 0.248\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 1.742\n",
      "  batch 200 loss: 1.630\n",
      "  batch 300 loss: 1.724\n",
      "  batch 400 loss: 1.593\n",
      "  batch 500 loss: 1.678\n",
      "TRAIN loss: 1.686, VALIDATION loss: 1.698, accuraccy: 0.230\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 1.634\n",
      "  batch 200 loss: 1.723\n",
      "  batch 300 loss: 1.732\n",
      "  batch 400 loss: 1.684\n",
      "  batch 500 loss: 1.690\n",
      "TRAIN loss: 1.686, VALIDATION loss: 1.701, accuraccy: 0.236\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 1.690\n",
      "  batch 200 loss: 1.733\n",
      "  batch 300 loss: 1.548\n",
      "  batch 400 loss: 1.786\n",
      "  batch 500 loss: 1.684\n",
      "TRAIN loss: 1.686, VALIDATION loss: 1.701, accuraccy: 0.244\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 1.600\n",
      "  batch 200 loss: 1.874\n",
      "  batch 300 loss: 1.775\n",
      "  batch 400 loss: 1.719\n",
      "  batch 500 loss: 1.714\n",
      "TRAIN loss: 1.686, VALIDATION loss: 1.706, accuraccy: 0.242\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 1.690\n",
      "  batch 200 loss: 1.730\n",
      "  batch 300 loss: 1.647\n",
      "  batch 400 loss: 1.710\n",
      "  batch 500 loss: 1.653\n",
      "TRAIN loss: 1.685, VALIDATION loss: 1.724, accuraccy: 0.233\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 1.575\n",
      "  batch 200 loss: 1.618\n",
      "  batch 300 loss: 1.652\n",
      "  batch 400 loss: 1.618\n",
      "  batch 500 loss: 1.749\n",
      "TRAIN loss: 1.687, VALIDATION loss: 1.698, accuraccy: 0.246\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 1.685\n",
      "  batch 200 loss: 1.732\n",
      "  batch 300 loss: 1.678\n",
      "  batch 400 loss: 1.550\n",
      "  batch 500 loss: 1.613\n",
      "TRAIN loss: 1.684, VALIDATION loss: 1.703, accuraccy: 0.241\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 1.823\n",
      "  batch 200 loss: 1.756\n",
      "  batch 300 loss: 1.681\n",
      "  batch 400 loss: 1.847\n",
      "  batch 500 loss: 1.721\n",
      "TRAIN loss: 1.686, VALIDATION loss: 1.721, accuraccy: 0.244\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 1.596\n",
      "  batch 200 loss: 1.676\n",
      "  batch 300 loss: 1.549\n",
      "  batch 400 loss: 1.749\n",
      "  batch 500 loss: 1.615\n",
      "TRAIN loss: 1.683, VALIDATION loss: 1.702, accuraccy: 0.244\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 1.699\n",
      "  batch 200 loss: 1.562\n",
      "  batch 300 loss: 1.730\n",
      "  batch 400 loss: 1.642\n",
      "  batch 500 loss: 1.544\n",
      "TRAIN loss: 1.685, VALIDATION loss: 1.707, accuraccy: 0.236\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 1.623\n",
      "  batch 200 loss: 1.741\n",
      "  batch 300 loss: 1.667\n",
      "  batch 400 loss: 1.539\n",
      "  batch 500 loss: 1.816\n",
      "TRAIN loss: 1.685, VALIDATION loss: 1.695, accuraccy: 0.244\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 1.745\n",
      "  batch 200 loss: 1.717\n",
      "  batch 300 loss: 1.753\n",
      "  batch 400 loss: 1.749\n",
      "  batch 500 loss: 1.634\n",
      "TRAIN loss: 1.685, VALIDATION loss: 1.709, accuraccy: 0.240\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 1.651\n",
      "  batch 200 loss: 1.675\n",
      "  batch 300 loss: 1.733\n",
      "  batch 400 loss: 1.890\n",
      "  batch 500 loss: 1.601\n",
      "TRAIN loss: 1.685, VALIDATION loss: 1.707, accuraccy: 0.245\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 1.642\n",
      "  batch 200 loss: 1.949\n",
      "  batch 300 loss: 1.581\n",
      "  batch 400 loss: 1.664\n",
      "  batch 500 loss: 1.669\n",
      "TRAIN loss: 1.682, VALIDATION loss: 1.742, accuraccy: 0.221\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 1.825\n",
      "  batch 200 loss: 1.541\n",
      "  batch 300 loss: 1.718\n",
      "  batch 400 loss: 1.646\n",
      "  batch 500 loss: 1.863\n",
      "TRAIN loss: 1.683, VALIDATION loss: 1.705, accuraccy: 0.248\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 1.786\n",
      "  batch 200 loss: 1.719\n",
      "  batch 300 loss: 1.576\n",
      "  batch 400 loss: 1.825\n",
      "  batch 500 loss: 1.790\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.710, accuraccy: 0.245\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 1.657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 1.778\n",
      "  batch 300 loss: 1.649\n",
      "  batch 400 loss: 1.809\n",
      "  batch 500 loss: 1.748\n",
      "TRAIN loss: 1.684, VALIDATION loss: 1.700, accuraccy: 0.240\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 1.692\n",
      "  batch 200 loss: 1.658\n",
      "  batch 300 loss: 1.792\n",
      "  batch 400 loss: 1.744\n",
      "  batch 500 loss: 1.699\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.700, accuraccy: 0.246\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 1.718\n",
      "  batch 200 loss: 1.774\n",
      "  batch 300 loss: 1.702\n",
      "  batch 400 loss: 1.619\n",
      "  batch 500 loss: 1.722\n",
      "TRAIN loss: 1.679, VALIDATION loss: 1.709, accuraccy: 0.247\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 1.693\n",
      "  batch 200 loss: 1.761\n",
      "  batch 300 loss: 1.529\n",
      "  batch 400 loss: 1.734\n",
      "  batch 500 loss: 1.550\n",
      "TRAIN loss: 1.683, VALIDATION loss: 1.702, accuraccy: 0.247\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 1.651\n",
      "  batch 200 loss: 1.597\n",
      "  batch 300 loss: 1.599\n",
      "  batch 400 loss: 1.910\n",
      "  batch 500 loss: 1.544\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.720, accuraccy: 0.231\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 1.631\n",
      "  batch 200 loss: 1.857\n",
      "  batch 300 loss: 1.787\n",
      "  batch 400 loss: 1.842\n",
      "  batch 500 loss: 1.930\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.713, accuraccy: 0.242\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 1.713\n",
      "  batch 200 loss: 1.621\n",
      "  batch 300 loss: 1.687\n",
      "  batch 400 loss: 1.705\n",
      "  batch 500 loss: 1.567\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.710, accuraccy: 0.241\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 1.689\n",
      "  batch 200 loss: 1.699\n",
      "  batch 300 loss: 1.679\n",
      "  batch 400 loss: 1.599\n",
      "  batch 500 loss: 1.743\n",
      "TRAIN loss: 1.682, VALIDATION loss: 1.709, accuraccy: 0.233\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 1.800\n",
      "  batch 200 loss: 1.472\n",
      "  batch 300 loss: 1.764\n",
      "  batch 400 loss: 1.648\n",
      "  batch 500 loss: 1.627\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.700, accuraccy: 0.240\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 1.756\n",
      "  batch 200 loss: 1.564\n",
      "  batch 300 loss: 1.641\n",
      "  batch 400 loss: 1.779\n",
      "  batch 500 loss: 1.539\n",
      "TRAIN loss: 1.679, VALIDATION loss: 1.698, accuraccy: 0.242\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 1.575\n",
      "  batch 200 loss: 1.571\n",
      "  batch 300 loss: 1.646\n",
      "  batch 400 loss: 1.717\n",
      "  batch 500 loss: 1.551\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.736, accuraccy: 0.244\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 1.734\n",
      "  batch 200 loss: 1.668\n",
      "  batch 300 loss: 1.623\n",
      "  batch 400 loss: 1.682\n",
      "  batch 500 loss: 1.680\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.700, accuraccy: 0.242\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 1.513\n",
      "  batch 200 loss: 1.718\n",
      "  batch 300 loss: 1.754\n",
      "  batch 400 loss: 1.572\n",
      "  batch 500 loss: 1.674\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.699, accuraccy: 0.242\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 1.711\n",
      "  batch 200 loss: 1.712\n",
      "  batch 300 loss: 1.758\n",
      "  batch 400 loss: 1.605\n",
      "  batch 500 loss: 1.565\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.716, accuraccy: 0.245\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 1.638\n",
      "  batch 200 loss: 1.794\n",
      "  batch 300 loss: 1.575\n",
      "  batch 400 loss: 1.719\n",
      "  batch 500 loss: 1.513\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.719, accuraccy: 0.232\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 1.731\n",
      "  batch 200 loss: 1.554\n",
      "  batch 300 loss: 1.713\n",
      "  batch 400 loss: 1.584\n",
      "  batch 500 loss: 1.710\n",
      "TRAIN loss: 1.679, VALIDATION loss: 1.722, accuraccy: 0.239\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 1.687\n",
      "  batch 200 loss: 1.632\n",
      "  batch 300 loss: 1.663\n",
      "  batch 400 loss: 1.681\n",
      "  batch 500 loss: 1.600\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.695, accuraccy: 0.248\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 1.653\n",
      "  batch 200 loss: 1.589\n",
      "  batch 300 loss: 1.726\n",
      "  batch 400 loss: 1.882\n",
      "  batch 500 loss: 1.540\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.726, accuraccy: 0.238\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 1.579\n",
      "  batch 200 loss: 2.000\n",
      "  batch 300 loss: 1.589\n",
      "  batch 400 loss: 1.606\n",
      "  batch 500 loss: 1.596\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.718, accuraccy: 0.243\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 1.695\n",
      "  batch 200 loss: 1.666\n",
      "  batch 300 loss: 1.829\n",
      "  batch 400 loss: 1.697\n",
      "  batch 500 loss: 1.687\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.717, accuraccy: 0.234\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 1.627\n",
      "  batch 200 loss: 1.457\n",
      "  batch 300 loss: 1.697\n",
      "  batch 400 loss: 1.766\n",
      "  batch 500 loss: 1.596\n",
      "TRAIN loss: 1.679, VALIDATION loss: 1.741, accuraccy: 0.242\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 1.694\n",
      "  batch 200 loss: 1.535\n",
      "  batch 300 loss: 1.726\n",
      "  batch 400 loss: 1.541\n",
      "  batch 500 loss: 1.583\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.719, accuraccy: 0.225\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 1.549\n",
      "  batch 200 loss: 1.985\n",
      "  batch 300 loss: 1.585\n",
      "  batch 400 loss: 1.643\n",
      "  batch 500 loss: 1.625\n",
      "TRAIN loss: 1.679, VALIDATION loss: 1.701, accuraccy: 0.246\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 1.643\n",
      "  batch 200 loss: 1.787\n",
      "  batch 300 loss: 1.853\n",
      "  batch 400 loss: 1.845\n",
      "  batch 500 loss: 1.700\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.744, accuraccy: 0.232\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 1.647\n",
      "  batch 200 loss: 1.680\n",
      "  batch 300 loss: 1.629\n",
      "  batch 400 loss: 1.744\n",
      "  batch 500 loss: 1.678\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.699, accuraccy: 0.248\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 1.707\n",
      "  batch 200 loss: 1.662\n",
      "  batch 300 loss: 1.623\n",
      "  batch 400 loss: 1.680\n",
      "  batch 500 loss: 1.491\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.705, accuraccy: 0.248\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 1.613\n",
      "  batch 200 loss: 2.402\n",
      "  batch 300 loss: 1.587\n",
      "  batch 400 loss: 1.811\n",
      "  batch 500 loss: 1.668\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.721, accuraccy: 0.244\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 1.805\n",
      "  batch 200 loss: 1.743\n",
      "  batch 300 loss: 1.907\n",
      "  batch 400 loss: 1.672\n",
      "  batch 500 loss: 1.800\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.710, accuraccy: 0.235\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 1.594\n",
      "  batch 200 loss: 1.512\n",
      "  batch 300 loss: 1.759\n",
      "  batch 400 loss: 1.814\n",
      "  batch 500 loss: 1.661\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.709, accuraccy: 0.245\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 1.972\n",
      "  batch 200 loss: 1.665\n",
      "  batch 300 loss: 1.718\n",
      "  batch 400 loss: 1.684\n",
      "  batch 500 loss: 1.553\n",
      "TRAIN loss: 1.674, VALIDATION loss: 1.742, accuraccy: 0.242\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 1.565\n",
      "  batch 200 loss: 1.646\n",
      "  batch 300 loss: 1.553\n",
      "  batch 400 loss: 1.587\n",
      "  batch 500 loss: 1.682\n",
      "TRAIN loss: 1.681, VALIDATION loss: 1.711, accuraccy: 0.240\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 1.655\n",
      "  batch 200 loss: 1.721\n",
      "  batch 300 loss: 1.691\n",
      "  batch 400 loss: 1.645\n",
      "  batch 500 loss: 1.754\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.738, accuraccy: 0.239\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 1.725\n",
      "  batch 200 loss: 1.698\n",
      "  batch 300 loss: 1.732\n",
      "  batch 400 loss: 1.840\n",
      "  batch 500 loss: 1.620\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.705, accuraccy: 0.241\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 1.530\n",
      "  batch 200 loss: 1.627\n",
      "  batch 300 loss: 1.814\n",
      "  batch 400 loss: 1.683\n",
      "  batch 500 loss: 1.583\n",
      "TRAIN loss: 1.675, VALIDATION loss: 1.701, accuraccy: 0.237\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 1.608\n",
      "  batch 200 loss: 1.481\n",
      "  batch 300 loss: 1.693\n",
      "  batch 400 loss: 1.644\n",
      "  batch 500 loss: 1.713\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.709, accuraccy: 0.243\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 1.697\n",
      "  batch 200 loss: 1.635\n",
      "  batch 300 loss: 1.610\n",
      "  batch 400 loss: 1.788\n",
      "  batch 500 loss: 2.319\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.718, accuraccy: 0.245\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 1.608\n",
      "  batch 200 loss: 1.763\n",
      "  batch 300 loss: 1.676\n",
      "  batch 400 loss: 1.715\n",
      "  batch 500 loss: 1.727\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.708, accuraccy: 0.243\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 1.578\n",
      "  batch 200 loss: 1.710\n",
      "  batch 300 loss: 1.611\n",
      "  batch 400 loss: 1.644\n",
      "  batch 500 loss: 1.744\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.702, accuraccy: 0.237\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 1.569\n",
      "  batch 200 loss: 1.737\n",
      "  batch 300 loss: 1.778\n",
      "  batch 400 loss: 1.647\n",
      "  batch 500 loss: 1.896\n",
      "TRAIN loss: 1.675, VALIDATION loss: 1.699, accuraccy: 0.242\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 1.904\n",
      "  batch 200 loss: 1.750\n",
      "  batch 300 loss: 1.595\n",
      "  batch 400 loss: 1.781\n",
      "  batch 500 loss: 1.564\n",
      "TRAIN loss: 1.675, VALIDATION loss: 1.719, accuraccy: 0.243\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 1.734\n",
      "  batch 200 loss: 1.667\n",
      "  batch 300 loss: 1.651\n",
      "  batch 400 loss: 1.563\n",
      "  batch 500 loss: 1.741\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.709, accuraccy: 0.245\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 1.707\n",
      "  batch 200 loss: 1.651\n",
      "  batch 300 loss: 1.635\n",
      "  batch 400 loss: 1.651\n",
      "  batch 500 loss: 1.643\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.711, accuraccy: 0.245\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 1.621\n",
      "  batch 200 loss: 1.620\n",
      "  batch 300 loss: 1.654\n",
      "  batch 400 loss: 1.616\n",
      "  batch 500 loss: 1.609\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.708, accuraccy: 0.245\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 1.638\n",
      "  batch 200 loss: 1.488\n",
      "  batch 300 loss: 1.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 400 loss: 1.618\n",
      "  batch 500 loss: 1.587\n",
      "TRAIN loss: 1.675, VALIDATION loss: 1.701, accuraccy: 0.237\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 1.667\n",
      "  batch 200 loss: 1.691\n",
      "  batch 300 loss: 1.608\n",
      "  batch 400 loss: 1.633\n",
      "  batch 500 loss: 1.656\n",
      "TRAIN loss: 1.673, VALIDATION loss: 1.703, accuraccy: 0.244\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 1.683\n",
      "  batch 200 loss: 1.805\n",
      "  batch 300 loss: 1.586\n",
      "  batch 400 loss: 2.045\n",
      "  batch 500 loss: 1.546\n",
      "TRAIN loss: 1.676, VALIDATION loss: 1.710, accuraccy: 0.236\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 1.792\n",
      "  batch 200 loss: 1.768\n",
      "  batch 300 loss: 1.722\n",
      "  batch 400 loss: 1.736\n",
      "  batch 500 loss: 1.670\n",
      "TRAIN loss: 1.680, VALIDATION loss: 1.697, accuraccy: 0.249\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 1.795\n",
      "  batch 200 loss: 1.528\n",
      "  batch 300 loss: 1.930\n",
      "  batch 400 loss: 1.608\n",
      "  batch 500 loss: 1.683\n",
      "TRAIN loss: 1.670, VALIDATION loss: 1.698, accuraccy: 0.247\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 1.757\n",
      "  batch 200 loss: 1.773\n",
      "  batch 300 loss: 1.733\n",
      "  batch 400 loss: 1.707\n",
      "  batch 500 loss: 1.734\n",
      "TRAIN loss: 1.678, VALIDATION loss: 1.726, accuraccy: 0.244\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 1.643\n",
      "  batch 200 loss: 1.671\n",
      "  batch 300 loss: 1.541\n",
      "  batch 400 loss: 1.753\n",
      "  batch 500 loss: 1.783\n",
      "TRAIN loss: 1.674, VALIDATION loss: 1.709, accuraccy: 0.246\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 1.632\n",
      "  batch 200 loss: 1.626\n",
      "  batch 300 loss: 1.562\n",
      "  batch 400 loss: 1.671\n",
      "  batch 500 loss: 1.585\n",
      "TRAIN loss: 1.674, VALIDATION loss: 1.700, accuraccy: 0.242\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 1.687\n",
      "  batch 200 loss: 1.728\n",
      "  batch 300 loss: 1.726\n",
      "  batch 400 loss: 1.544\n",
      "  batch 500 loss: 1.722\n",
      "TRAIN loss: 1.674, VALIDATION loss: 1.712, accuraccy: 0.243\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 1.539\n",
      "  batch 200 loss: 1.666\n",
      "  batch 300 loss: 1.800\n",
      "  batch 400 loss: 1.617\n",
      "  batch 500 loss: 1.641\n",
      "TRAIN loss: 1.672, VALIDATION loss: 1.708, accuraccy: 0.245\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 1.829\n",
      "  batch 200 loss: 1.663\n",
      "  batch 300 loss: 1.721\n",
      "  batch 400 loss: 1.552\n",
      "  batch 500 loss: 1.712\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.706, accuraccy: 0.237\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 1.625\n",
      "  batch 200 loss: 1.743\n",
      "  batch 300 loss: 1.579\n",
      "  batch 400 loss: 1.587\n",
      "  batch 500 loss: 1.843\n",
      "TRAIN loss: 1.673, VALIDATION loss: 1.721, accuraccy: 0.243\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 1.622\n",
      "  batch 200 loss: 1.697\n",
      "  batch 300 loss: 1.680\n",
      "  batch 400 loss: 1.699\n",
      "  batch 500 loss: 1.677\n",
      "TRAIN loss: 1.677, VALIDATION loss: 1.706, accuraccy: 0.243\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 1.609\n",
      "  batch 200 loss: 1.616\n",
      "  batch 300 loss: 1.675\n",
      "  batch 400 loss: 1.615\n",
      "  batch 500 loss: 1.679\n",
      "TRAIN loss: 1.670, VALIDATION loss: 1.705, accuraccy: 0.239\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # One training step\n",
    "    avg_loss = train_one_epoch(model, loss_fn, optimizer, trainloader)\n",
    "\n",
    "    # Validation performance\n",
    "    running_cum_vloss = 0.0\n",
    "    vcorrect = 0\n",
    "    for i, vdata in enumerate(validloader):\n",
    "        vinputs, vlabels = vdata\n",
    "        with torch.no_grad():\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "        running_cum_vloss += vloss * vinputs.shape[0]\n",
    "        # count the correctly classified samoples\n",
    "        vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "    # Get average loss and accuraccy\n",
    "    avg_vloss = running_cum_vloss / len(x_val)\n",
    "    vacc = vcorrect / len(x_val)\n",
    "    \n",
    "    print(f\"TRAIN loss: {avg_loss:.3f}, VALIDATION loss: {avg_vloss:.3f}, accuraccy: {vacc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11200,)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.zeros(len(x_test))\n",
    "test_y = np.zeros(len(x_test))\n",
    "print(test_predictions.shape)\n",
    "ii = 0\n",
    "for i, vdata in enumerate(testloader):\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    test_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).numpy()\n",
    "    test_y[ii:(ii + vinputs.shape[0])] = vlabels.numpy()\n",
    "    ii += vinputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testovací přesnost: 0.24330357142857142\n",
      "[[   0    0    0 1084    0    0    7    1    4    8]\n",
      " [   0    0    0 1115    0    0    0    0    0    1]\n",
      " [   5    0    0 1013    0    0   20    0    2   25]\n",
      " [   4    0    0 1081    0    0   18    0    1    5]\n",
      " [   1    0    0 1106    0    0   14    0    1    3]\n",
      " [   5    0    0   39    0    0   59   85  597  414]\n",
      " [   5    0    0 1022    0    0   21    0    6   30]\n",
      " [   0    0    0    1    0    0    3  676  419   32]\n",
      " [   1    0    0   81    0    0   58  141  596  225]\n",
      " [   1    0    0   46    0    0   37   52  678  351]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Testovací přesnost: {accuracy_score(test_y, test_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_y, test_predictions)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
